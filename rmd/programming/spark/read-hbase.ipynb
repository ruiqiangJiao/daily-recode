{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: 读取Hbase\n",
        "---"
      ],
      "id": "aa5a7e79"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 安装包\n",
        "\n",
        "### python 包\n",
        "\n",
        "- pyspark==3.1.1\n",
        "\n",
        "- protobuf==3.17.0\n",
        "\n",
        "- py4j==0.10.9\n",
        "\n",
        "### jar 包\n",
        "\n",
        "- hbase-common-2.0.0.3.0.0.0-1634.jar\n",
        "\n",
        "- hbase-client-2.0.0.3.0.0.0-1634.jar\n",
        "\n",
        "- hbase-mapreduce-2.0.0.3.0.0.0-1634.jar\n",
        "\n",
        "- hbase-shaded-miscellaneous-2.1.0.jar\n",
        "\n",
        "- hbase-protocol-2.0.0.3.0.0.0-1634.jar\n",
        "\n",
        "- hbase-protocol-shaded-2.0.0.3.0.0.0-1634.jar\n",
        "\n",
        "- hbase-shaded-protobuf-2.1.0.jar\n",
        "\n",
        "- hbase-shaded-netty-2.1.0.jar\n",
        "\n",
        "- spark-examples_2.11-1.6.0-typesafe-001.jar\n",
        "\n",
        "  查询 Hbase 得到的 rowKey 类型 ImmutableBytesWritable 和结果做类型转换成 string\n",
        "\n",
        "上面的 jar 包放置在 spark-script/lib/python3.7/site-packages/pyspark/jars 目录下面\n",
        "\n",
        "## 测试样例\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "import TerminalLocationReport_pb2 as TerminalLocationReportProto\n",
        "import json\n",
        "import sys, os\n",
        "import codecs\n",
        "\n",
        "def get_rdd_element(rdd):\n",
        "    row_values = json.loads(rdd[1])['value']\n",
        "    a = codecs.escape_decode(row_values.encode())[0]                \n",
        "    report = TerminalLocationReportProto.TerminalLocationReport()\n",
        "    report.ParseFromString(a)\n",
        "    return report\n",
        "\n",
        "def get_hbase_terminal_info():\n",
        "    \"\"\"\n",
        "    hbase_conf 中的参数配置\n",
        "    zookeeper集群地址 \n",
        "    hbase master的元数据信息都存在zookeeper中，读取zookeeper就能锁定你要读的数据在hdfs的哪个位置；\n",
        "    指定到zookeeper的hbase原数据目录下去读；读取的表名\n",
        "    \n",
        "    \"\"\"\n",
        "    hbase_conf = {\n",
        "                 'hbase.zookeeper.quorum':'b-bd-cd1-01.local.lan,b-bd-cd1-02.local.lan,b-bd-cd1-03.local.lan',                                      \n",
        "                 'zookeeper.znode.parent': '/hbase-unsecure',\n",
        "                 'hbase.mapreduce.inputtable': ''\n",
        "    }\n",
        "    \n",
        "    \"\"\"\n",
        "    newAPIHadoopRDD用sparkContext读取hbase表，依次配置：指定读取表用TableInputFormat类；\n",
        "    rowkey的返回类型: 数据返回类型；rowkey返回类型强制转换为string；数据返回类型强制转换为string\n",
        "    \"\"\"\n",
        "\n",
        "    data_rdd = sc.newAPIHadoopRDD('org.apache.hadoop.hbase.mapreduce.TableInputFormat',\n",
        "                                   'org.apache.hadoop.hbase.io.ImmutableBytesWritable',\n",
        "                                   'org.apache.hadoop.hbase.client.Result',\n",
        "                                   'org.apache.spark.examples.pythonconverters.ImmutableBytesWritableToStringConverter',\n",
        "                                   'org.apache.spark.examples.pythonconverters.HBaseResultToStringConverter',\n",
        "                              conf=hbase_conf)\\\n",
        "                  .map(get_rdd_element)\n",
        "    return data_rdd\n",
        "    \n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "   # add config\n",
        "   conf = SparkConf()                                # 初始化 \n",
        "   conf.setAppName('read-hbase')                     # 设置程序名称\n",
        "   # 设置 spark 采取 Kryo 序列化\n",
        "   conf.set('spark.serializer', 'org.apache.spark.serializer.KryoSerializer')\n",
        "   conf.set('spark.kryoserializer.buffer.max', '512m')\n",
        "   # spark_session 用于读取多种数据源(mysql、hdfs、Hive)\n",
        "   spark_session = SparkSession.builder.config(conf=conf).getOrCreate()\n",
        "   # spark_session 中的 sparkContext 对象可读取 hbase、hdfs\n",
        "   sc = spark_session.sparkContext\n",
        "   rdd_data = get_hbase_terminal_info()\n",
        "   print(rdd_data.take(1))\n",
        "\n",
        "\n",
        "```\n"
      ],
      "id": "81cb3506"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5
}