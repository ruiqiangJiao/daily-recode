<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head>
  <meta charset="utf-8">
  <meta name="generator" content="quarto-(Local Development)">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>日常记录 – 服务器提交运行任务</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>

  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
  <script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
  <meta name="quarto:offset" content="../../../">
  <script src="../../../site_libs/quarto-search/autocomplete.min.js"></script>
  <script src="../../../site_libs/quarto-search/fuse.min.js"></script>
  <script src="../../../site_libs/quarto-search/quarto-search.js"></script>
  <link href="../../../rmd/programming/spark/hadoop.html" rel="next">
  <link href="../../../rmd/programming/spark/read-mysql.html" rel="prev">
  <script src="../../../site_libs/quarto-html/quarto.js"></script>
  <script src="../../../site_libs/quarto-html/popper.min.js"></script>
  <script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
  <script src="../../../site_libs/quarto-html/clipboard.min.js"></script>
  <script src="../../../site_libs/quarto-html/anchor.min.js"></script>
  <link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
  <link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet">
  <script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
  <link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
  <link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet">
  <link rel="stylesheet" href="../../../styles.css">
</head>
<body>
<div id="quarto-search-results"></div>
<header id="quarto-header" class="headroom fixed-top">
<nav class="navbar navbar-expand-lg navbar-dark bg-primary ">
    <div class="container-fluid">
  <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">日常记录</span>
  </a>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
      <div class="collapse navbar-collapse" id="navbarCollapse">
        <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html">主页</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html">关于</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../refrence.html">参考</a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 编程
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-">    
        <li class="dropdown-header">
 python</li>
        <li>
    <a class="dropdown-item" href="../../../rmd/programming/python/read-redis-cluster.html">
 读取 Redis 集群</a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../rmd/programming/python/read-mysql.html">
 读取 MySQL</a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../rmd/programming/python/frequent-function.html">
 常用方法</a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../rmd/programming/python/pytorch.html">
 pytorch 参考</a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">
 R</li>
        <li>
    <a class="dropdown-item" href="../../../rmd/programming/R/about.html">
 常用方法</a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">
 git</li>
        <li>
    <a class="dropdown-item" href="../../../rmd/programming/git/refrence.html">
 精通Git(第二版简体中文)</a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">
 spark</li>
        <li>
    <a class="dropdown-item" href="../../../rmd/programming/spark/install.html">
 安装</a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../rmd/programming/spark/read-hbase.html">
 读取Hbase</a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../rmd/programming/spark/read-mysql.html">
 读取 MySQL</a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../rmd/programming/spark/submit-run-task.html">
 服务器提交运行任务</a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../rmd/programming/spark/hadoop.html">
 hadoop 常用命令</a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../rmd/programming/spark/hive.html">
 Hive</a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">
 SQL</li>
        <li>
    <a class="dropdown-item" href="../../../rmd/programming/SQL/MySQL.html">
 MySQL</a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">
 Leet code</li>
        <li>
    <a class="dropdown-item" href="../../../rmd/programming/leetcode/refrence.html">
 Leet code 参考资料</a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">
 软件</li>
        <li>
    <a class="dropdown-item" href="../../../rmd/programming/software/vim.html">
 Vim</a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../rmd/programming/software/latex.html">
 LaTex</a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu--1" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 算法
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu--1">    
        <li class="dropdown-header">
 统计学</li>
        <li>
    <a class="dropdown-item" href="../../../rmd/algorithm/statistics/elements_of_statistical_learning.html">
 The Elements of Statistical Learning</a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../rmd/algorithm/statistics/isolation_forest.html">
 孤立森林</a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../rmd/algorithm/statistics/lof.html">
 局部离群因子</a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../rmd/algorithm/statistics/XGboost.html">
 XGboost</a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">
 强化学习</li>
        <li>
    <a class="dropdown-item" href="../../../rmd/algorithm/reinforcement_learning/about.html">
 参考文档</a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">
 深度学习</li>
        <li>
    <a class="dropdown-item" href="../../../rmd/algorithm/deep_learning/dive_into_deep_learning.html">
 动手学深度学习</a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">
 其它</li>
    </ul>
  </li>
</ul>
          <form class="d-flex mb-0">
  <input id="quarto-search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>
      </div> <!-- /navcollapse -->
  </div> <!-- /container-fluid -->
</nav>
<nav class="quarto-secondary-nav py-2 d-lg-none d-md-block " data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <div class="container-fluid d-flex justify-content-between">
    <h1 class="quarto-secondary-nav-title mb-0 fs-3 d-inline">服务器提交运行任务</h1>
    <button type="button" class="quarto-btn-toggle btn d-lg-none py-0 px-1 d-inline-block border-0 ">
      <i class="bi bi-chevron-right"></i>
    </button>
  </div>
</nav>
</header>
 <!-- /navbar/sidebar -->
<div class="container-fluid quarto-container d-flex flex-column page-layout-article">
<div class="row flex-fill" id="quarto-content">
  <div id="quarto-sidebar" class="col col-12 col-lg-3 d-lg-flex px-0 pt-0 sidebar collapse sidebar-navigation">
    <nav class="me-lg-auto py-2 docked  overflow-scroll">
  <div class="sidebar-menu-container"> 
  <ul class="list-unstyled mt-1 px-3">
      <ul class="list-unstyled sidebar-section depth1">
    <li class="">
        <div class="me-auto ">
            <a class="sidebar-section-item d-inline-flex text-start w-100 " data-bs-toggle="collapse" data-bs-target="#python-collapse" aria-expanded="true">
              <div class="me-auto  sidebar-section-item">python</div>
            <div><i class="bi bi-chevron-right ms-2 "></i></div>
            </a>
        </div>
      <div class="collapse  show" id="python-collapse">
        <ul class="list-unstyled sidebar-item-contents">
          <li class="sidebar-item">
  <a href="../../../rmd/programming/python/read-redis-cluster.html" class="">读取 Redis 集群</a>
</li>
          <li class="sidebar-item">
  <a href="../../../rmd/programming/python/read-mysql.html" class="">读取 MySQL</a>
</li>
          <li class="sidebar-item">
  <a href="../../../rmd/programming/python/frequent-function.html" class="">常用方法</a>
</li>
          <li class="sidebar-item">
  <a href="../../../rmd/programming/python/pytorch.html" class="">pytorch 参考</a>
</li>
        </ul>
      </div>
    </li>
  </ul>
      <ul class="list-unstyled sidebar-section depth1">
    <li class="">
        <div class="me-auto ">
            <a class="sidebar-section-item d-inline-flex text-start w-100 " data-bs-toggle="collapse" data-bs-target="#r-collapse" aria-expanded="true">
              <div class="me-auto  sidebar-section-item">R</div>
            <div><i class="bi bi-chevron-right ms-2 "></i></div>
            </a>
        </div>
      <div class="collapse  show" id="r-collapse">
        <ul class="list-unstyled sidebar-item-contents">
          <li class="sidebar-item">
  <a href="../../../rmd/programming/R/about.html" class="">常用方法</a>
</li>
        </ul>
      </div>
    </li>
  </ul>
      <ul class="list-unstyled sidebar-section depth1">
    <li class="">
        <div class="me-auto ">
            <a class="sidebar-section-item d-inline-flex text-start w-100 " data-bs-toggle="collapse" data-bs-target="#git-collapse" aria-expanded="true">
              <div class="me-auto  sidebar-section-item">git</div>
            <div><i class="bi bi-chevron-right ms-2 "></i></div>
            </a>
        </div>
      <div class="collapse  show" id="git-collapse">
        <ul class="list-unstyled sidebar-item-contents">
          <li class="sidebar-item">
  <a href="../../../rmd/programming/git/refrence.html" class="">精通Git(第二版简体中文)</a>
</li>
        </ul>
      </div>
    </li>
  </ul>
      <ul class="list-unstyled sidebar-section depth1">
    <li class="">
        <div class="me-auto ">
            <a class="sidebar-section-item d-inline-flex text-start w-100 " data-bs-toggle="collapse" data-bs-target="#spark-collapse" aria-expanded="true">
              <div class="me-auto  sidebar-section-item">spark</div>
            <div><i class="bi bi-chevron-right ms-2 "></i></div>
            </a>
        </div>
      <div class="collapse  show" id="spark-collapse">
        <ul class="list-unstyled sidebar-item-contents">
          <li class="sidebar-item">
  <a href="../../../rmd/programming/spark/install.html" class="">安装</a>
</li>
          <li class="sidebar-item">
  <a href="../../../rmd/programming/spark/read-hbase.html" class="">读取Hbase</a>
</li>
          <li class="sidebar-item">
  <a href="../../../rmd/programming/spark/read-mysql.html" class="">读取 MySQL</a>
</li>
          <li class="sidebar-item">
  <a href="../../../rmd/programming/spark/submit-run-task.html" class="active">服务器提交运行任务</a>
</li>
          <li class="sidebar-item">
  <a href="../../../rmd/programming/spark/hadoop.html" class="">hadoop 常用命令</a>
</li>
          <li class="sidebar-item">
  <a href="../../../rmd/programming/spark/hive.html" class="">Hive</a>
</li>
        </ul>
      </div>
    </li>
  </ul>
      <ul class="list-unstyled sidebar-section depth1">
    <li class="">
        <div class="me-auto ">
            <a class="sidebar-section-item d-inline-flex text-start w-100 " data-bs-toggle="collapse" data-bs-target="#sql-collapse" aria-expanded="true">
              <div class="me-auto  sidebar-section-item">SQL</div>
            <div><i class="bi bi-chevron-right ms-2 "></i></div>
            </a>
        </div>
      <div class="collapse  show" id="sql-collapse">
        <ul class="list-unstyled sidebar-item-contents">
          <li class="sidebar-item">
  <a href="../../../rmd/programming/SQL/MySQL.html" class="">MySQL</a>
</li>
        </ul>
      </div>
    </li>
  </ul>
      <ul class="list-unstyled sidebar-section depth1">
    <li class="">
        <div class="me-auto ">
            <a class="sidebar-section-item d-inline-flex text-start w-100 " data-bs-toggle="collapse" data-bs-target="#leetcode-collapse" aria-expanded="true">
              <div class="me-auto  sidebar-section-item">Leet code</div>
            <div><i class="bi bi-chevron-right ms-2 "></i></div>
            </a>
        </div>
      <div class="collapse  show" id="leetcode-collapse">
        <ul class="list-unstyled sidebar-item-contents">
          <li class="sidebar-item">
  <a href="../../../rmd/programming/leetcode/refrence.html" class="">Leet code 参考资料</a>
</li>
        </ul>
      </div>
    </li>
  </ul>
      <ul class="list-unstyled sidebar-section depth1">
    <li class="">
        <div class="me-auto ">
            <a class="sidebar-section-item d-inline-flex text-start w-100 " data-bs-toggle="collapse" data-bs-target="#软件-collapse" aria-expanded="true">
              <div class="me-auto  sidebar-section-item">软件</div>
            <div><i class="bi bi-chevron-right ms-2 "></i></div>
            </a>
        </div>
      <div class="collapse  show" id="软件-collapse">
        <ul class="list-unstyled sidebar-item-contents">
          <li class="sidebar-item">
  <a href="../../../rmd/programming/software/vim.html" class="">Vim</a>
</li>
          <li class="sidebar-item">
  <a href="../../../rmd/programming/software/latex.html" class="">LaTex</a>
</li>
        </ul>
      </div>
    </li>
  </ul>
  </ul>
  </div>
</nav>
  </div>
  <div id="quarto-toc-sidebar" class="col col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-toc order-last"><nav id="TOC" role="doc-toc">
<h2 id="toc-title">On this page</h2>
<ul>
<li><a href="#local模式" class="nav-link active" data-scroll-target="#local模式">local模式</a></li>
<li><a href="#standalone模式" class="nav-link" data-scroll-target="#standalone模式">standalone模式</a></li>
<li><a href="#yarn-client模式" class="nav-link" data-scroll-target="#yarn-client模式">yarn client模式</a></li>
<li><a href="#yarn-cluster模式" class="nav-link" data-scroll-target="#yarn-cluster模式">yarn cluster模式</a></li>
<li><a href="#mesos模式" class="nav-link" data-scroll-target="#mesos模式">mesos模式</a>
<ul class="collapse">
<li><a href="#spark提交任务参数说明" class="nav-link" data-scroll-target="#spark提交任务参数说明">Spark提交任务参数说明</a></li>
</ul></li>
</ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/ruiqiangJiao/daily-recode/edit/main/rmd\programming\spark\submit-run-task.qmd">Edit this page</a></p></div></div></nav></div>
  <div class="col mx-auto col-sm-12 col-md-9 col-lg-7 col-xl-7 px-lg-4 pe-xxl-4 ps-xxl-0">
<main>
<header id="title-block-header">
<h1 class="title d-none d-lg-block display-7">服务器提交运行任务</h1>
</header>

<p>目前我们提交并运行pyspark的模式目前有5种，下面我们依次说明每一种模式.</p>
<section id="local模式" class="level2">
<h2 class="anchored" data-anchor-id="local模式">local模式</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>spark<span class="op">-</span>submit <span class="op">--</span>master local[<span class="op">*</span>] <span class="op">--</span>num<span class="op">-</span>executors <span class="dv">20</span> <span class="op">--</span>executor<span class="op">-</span>cores <span class="dv">2</span> <span class="op">--</span>executor<span class="op">-</span>memory <span class="dv">10</span>g <span class="op">--</span>driver<span class="op">-</span>memory <span class="dv">6</span>g <span class="op">--</span>conf <span class="st">"spark.pyspark.python=/usr/bin/python3.5"</span> <span class="op">--</span>archives <span class="op">/</span>usr<span class="op">/</span>local<span class="op">/</span>lib<span class="op">/</span>python3<span class="fl">.5</span><span class="op">/</span>pyspark_venv.tar.gz<span class="co">#environment /data/read_hbase.py</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>提交命令中使用的–master local[n]或者是–master local[*]，该种模式提交的spark任务只会在当前机器上运行，这种模式运行spark任务，你在机器上看到只有一个进程，日志打出：</p>
<p>21/08/01 08:28:15 WARN Utils: Service ‘SparkUI’ could not bind on port 4040. Attempting port 4041.</p>
<p>说明该任务的ui界面默认是4040端口展示，但是4040端口被占用了，所以用了4041端口来启动ui界面，用当前机器ip加端口就能看到该任务的ui图。就我们的机器而言就是ip:4041。进入ui界面，可以看到自己的spark任务的dag图，验证前面所说的只有一个进程，点击executors，就可以看到一共启动了多少个进程去运行该任务：</p>
<p>可以看到只有一个driver进程，就在b-anal-cd1-01也就是241上启动的，分配了72个cores，计算出来的storge momory是3.2g。看到这个ui，你可能就用疑问了，为什么是72cores，storge momory为什么是3.2g?</p>
<p>原因是由于我们用的是–master local[*]（local模式启动）其中*代表当前机器有多少core就会启动多少个线程，local[n]表示启动n个，至于storge momory其实是根据spark的内存模型算出来的。local模式下spark-submit进程既当爹又当妈，既是提交任务的client进程，又是driver进程，还是task的executor（spark中driver是用于管理executor，将任务分发给executor执行，executor执行完后通知driver）。所以提交参数中–num-executors 20 –executor-cores 2 –executor-memory 10g这个几个参数是无效的，也是不需要的。–driver-memory 6g设置了driver的总内存6g，算下来storge momory是3.2g。</p>
<p>该种模式下玩的是单进程，启动多线程的方式来模拟了spark启动多个executor去执行spark的子任务，所以我们需要把driver的内存配大到yarn cluster模式下多个executor内存和的大小。</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>spark<span class="op">-</span>submit <span class="op">--</span>master local[<span class="op">*</span>] <span class="op">--</span>driver<span class="op">-</span>memory <span class="dv">100</span>g <span class="op">--</span>conf <span class="st">"spark.pyspark.python=/usr/bin/python3.5"</span> <span class="op">--</span>archives <span class="op">/</span>usr<span class="op">/</span>local<span class="op">/</span>lib<span class="op">/</span>python3<span class="fl">.5</span><span class="op">/</span>pyspark_venv.tar.gz<span class="co">#environmen /data/read_hbase.py</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="../../../img/programming/spark/04_ui.png" class="img-fluid"></p>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>该模式属于当前机器本地运行spark任务，仅限于数据比较少，几十个g或者是测试的情况下</p>
</div>
</div>
</section>
<section id="standalone模式" class="level2">
<h2 class="anchored" data-anchor-id="standalone模式">standalone模式</h2>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>spark<span class="op">-</span>submit <span class="op">--</span>master <span class="op">--</span>num<span class="op">-</span>executors <span class="dv">15</span> <span class="op">--</span>total<span class="op">-</span>executor<span class="op">-</span>cores<span class="op">=</span><span class="dv">40</span> <span class="op">--</span>executor<span class="op">-</span>cores <span class="dv">2</span> <span class="op">--</span>executor<span class="op">-</span>spark:<span class="op">//</span>b<span class="op">-</span>anal<span class="op">-</span>cd1<span class="op">-</span><span class="dv">01</span>:<span class="dv">7077</span>memory <span class="dv">15</span> <span class="op">--</span>driver<span class="op">-</span>memory <span class="dv">5</span>g <span class="op">--</span>conf <span class="st">"spark.pyspark.python=/usr/bin/python3.5"</span><span class="op">--</span>archives  <span class="op">--</span>py<span class="op">-</span>files <span class="op">/</span>data<span class="op">/*</span>.py <span class="op">/</span>data<span class="op">/</span>read_hbase.py</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>spark的集群模式，什么意思啦？也就是说spark这个组件可以维护一个集群，通过spark-submit可以将任务提交到集群中去运行。该集群由一个master和多个worker组成，submit提交任务给master后，master会根据参数在自己管理的所有worker节点上启动30个executor和一个driver，同时worker会给每一个启动的executor分配15g的内存。</p>
<p>目前只有一台服务器 A，在上面启动了master和worker，那么就维护了一个只有一台机器的spark集群，master和worker1都是当前机器，worker1就会启动30个executor和一个driver，同时worker会给每一个启动的executor分配15g的内存</p>
<p>如果再分配一台服务器B，我在上面只需启动worker就可以，那么就维护了一个两台机器的spark集群，master和worker1在A机器上worker2在B机器上，你提交任务过来的时候，worker1和worker2就会一起去完成启动30个executor和一个driver，同时worker会给每一个启动的executor分配15g的内存。可能就是worker1启动一个driver和10个executor，worker2启动剩余的20个executor。</p>
<p>使用该种模式，首先需要去创建一个spark集群, 然后在/usr/hdp/3.0.0.0-1634/spark2目录下，</p>
<ol type="1">
<li><p>执行./sbin/start-master.sh启动master，可以在默认端口ip:8080的master的ui界面看到集群状况</p></li>
<li><p>执行./sbin/start-slave.sh -c 30 -m 200G 启动worker，可以使用机器的30个core和200个g的内存用来分配给spark://ip:7077executor或driver执行任务。执行后在master的ui上可以看到一个worker加入了集群：</p></li>
</ol>
<p><img src="../../../img/programming/spark/04_standalone_ui.png" class="img-fluid"></p>
<p>如果还有别的机器，在那台机器上也执行一下：./sbin/start-slave.sh-c 30 -m 200G，ui上又一个worker也加入了spark://ip:7077进来。从例子看，目前这个集群最多只有200个g的内存和30个core提供任务计算，如果都用完，任务将提交不了。</p>
<p>该模式下用到了–total-executor-cores=40和–executor-cores 2来控制executor的数量，参数表示总共用40个cores，每个executor用2个cores，所以会启动20个executor。而–num-executors参数也就没有效果了</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>spark<span class="op">-</span>submit <span class="op">--</span>master<span class="op">--</span>total<span class="op">-</span>executor<span class="op">-</span>cores<span class="op">=</span><span class="dv">40</span> <span class="op">--</span>executor<span class="op">-</span>cores <span class="dv">2</span> <span class="op">--</span>executor<span class="op">-</span>memory <span class="dv">15</span>g <span class="op">--</span>driver<span class="op">-</span>spark:<span class="op">//</span>b<span class="op">-</span>anal<span class="op">-</span>cd1<span class="op">-</span><span class="dv">01</span>:<span class="dv">7077</span>memory <span class="dv">5</span>g <span class="op">--</span>conf <span class="st">"spark.pyspark.python=/usr/bin/python3.5"</span><span class="op">--</span>archives <span class="op">/</span>home<span class="op">/</span>test_pyspark_venv.tar.gz<span class="co">#environment --py-files /data/protobuf_to_dict.py /data/read_hbase.py</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="../../../img/programming/spark/04_standalone.png" class="img-fluid"></p>
</section>
<section id="yarn-client模式" class="level2">
<h2 class="anchored" data-anchor-id="yarn-client模式">yarn client模式</h2>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>spark<span class="op">-</span>submit <span class="op">--</span>master yarn <span class="op">--</span>queue llap <span class="op">--</span>name read<span class="op">-</span>hbase<span class="op">-</span>workflow <span class="op">--</span>num<span class="op">-</span>executors <span class="dv">15</span> <span class="op">--</span>executor<span class="op">-</span>cores <span class="dv">2</span> <span class="op">--</span>executor<span class="op">-</span>memory <span class="dv">15</span>g <span class="op">--</span>driver<span class="op">-</span>memory <span class="dv">5</span>g <span class="op">--</span>conf <span class="st">"spark.pyspark.python=/usr/bin/python3.5"</span> <span class="op">--</span>conf spark.driver.extraClassPath<span class="op">=/</span>usr<span class="op">/</span>hdp<span class="op">/</span>current<span class="op">/</span>phoenix<span class="op">-</span>client<span class="op">/</span>phoenix<span class="op">-</span>client.jar:<span class="op">/</span>usr<span class="op">/</span>hdp<span class="op">/</span>current<span class="op">/</span>spark2<span class="op">-</span>client<span class="op">/</span>jars<span class="op">/</span>spark<span class="op">-</span>hive_2<span class="fl">.11</span><span class="op">-</span><span class="fl">2.3.1.3.0.0.0</span><span class="op">-</span><span class="ot">1634.j</span>ar:<span class="op">/</span>usr<span class="op">/</span>hdp<span class="op">/</span>current<span class="op">/</span>spark2<span class="op">-</span> <span class="op">--</span>conf spark.executor.extraClassPath<span class="op">=/</span>usr<span class="op">/</span>hdp<span class="op">/</span>current<span class="op">/</span>phoenix<span class="op">-</span>clientclient<span class="op">/</span>jars<span class="op">/</span>spark<span class="op">-</span>examples_2<span class="fl">.11</span><span class="op">-</span><span class="fl">1.6.0</span><span class="op">-</span>typesafe<span class="op">-</span><span class="ot">001.j</span>ar<span class="op">/</span>phoenix<span class="op">-</span>client.jar:<span class="op">/</span>usr<span class="op">/</span>hdp<span class="op">/</span>current<span class="op">/</span>spark2<span class="op">-</span>client<span class="op">/</span>jars<span class="op">/</span>spark<span class="op">-</span>hive_2<span class="fl">.11</span><span class="op">-</span><span class="fl">2.3.1.3.0.0.0</span><span class="op">-</span><span class="ot">1634.j</span>ar:<span class="op">/</span>usr<span class="op">/</span>hdp<span class="op">/</span>current<span class="op">/</span>spark2<span class="op">-</span>client <span class="op">--</span>archives <span class="op">/</span>home<span class="op">/</span>klicenbd<span class="op">/</span>jiaoruiqiang<span class="op">/</span>test_pyspark_venv.tar.gz<span class="co">#environment --/jars/spark-examples_2.11-1.6.0-typesafe-001.jarpy-files /data/protobuf_to_dict.py /data/read_hbase.py</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>这种区别于spark集群模式，首先是–master yarn参数改变，设置了–queue llap参数表示该任务提交到yarn后用llap对列的资源，也可以写成offline，具体有没有资源要看一下yarn的资源状况</p>
<p>http://b-bd-cd1-01.local.lan:8088/ui2/index.html#/yarn-queues/root 还对spark.driver.extraClassPath和spark.executor.extraClassPath做了设置,表示driver和executor启动的时候需要把这些jar包拷到自己的容器里面去用</p>
<p>yarn和spark集群模式的区别是，yarn是一个对机器资源的管理者，当提交参数是–master yarn时，–deploy-mode参数没写默认时client，表示用yarn client模式启动任务。此时提交的任务就会向yarn的一个做资源管理的组件resource manager申请你提交参数中的资源，比如我们的命令就是：我需要启动15个executor，每个executor需要分15g的内存，driver分5g的内存。resource manager就会给yarn的另一个管理集群机器的组件node manager发送指令，选择一个在当前node manager机器上启动application master，再向node managerresource manager注册。接着applicati再去resource manager申请executor的资源，申请到后与对应机器的node manager联系，启动executor，启动好后driver分发任务给on masterexecutor执行。区别于spark集群模式，向自己集群中的master提交并申请资源，同时worker节点去启动executor。</p>
<p>client模式的默认端口也是4040，打开后就可以看到当前任务的ui界面了，我们还是来看executors界面：</p>
<p>yarn和spark集群模式的区别是，yarn是一个对机器资源的管理者，当提交参数是–master yarn时，–deploy-mode参数没写默认时client，表示用yarn client模式启动任务。此时提交的任务就会向yarn的一个做资源管理的组件resource manager申请你提交参数中的资源，比如我们的命令就是：我需要启动15个executor，每个executor需要分15g的内存，driver分5g的内存。resource manager就会给yarn的另一个管理集群机器的组件node manager发送指令，选择一个在当前node manager机器上启动application master，再向node managerresource manager注册。接着applicati再去resource manager申请executor的资源，申请到后与对应机器的node manager联系，启动executor，启动好后driver分发任务给on masterexecutor执行。区别于spark集群模式，向自己集群中的master提交并申请资源，同时worker节点去启动executor。</p>
<p>client模式的默认端口也是4040，打开后就可以看到当前任务的ui界面了，我们还是来看executors界面</p>
<p>可以和上面的spark集群模式的yarn做对比：–num-executors 15 –executor-cores 2 –executor-memory 15g –driver-memory 5g这几个参数一样申请到的资源也是一样的。区别在于yarn client模式下，只有driver容器（进程）启动在当前机器上，其余的executor已经在大数据的其他机器上了。这样读取数据的速度相对更快，因为要读取的数据很可能和executor在一台机器上，所以更快。</p>
<p>同时提交的任务已经可以在yarn的管理任务列表中看到了，：–name read-hbase-workflow名字和这个参数设置的一样，也可以从yarn界面点进去了yarn的任务管理界面：或http://b-bd-cd1-01.local.lan:8088/clusterhttp://b-bd-cd1-01.local.lan:8088/ui2/index.html#/yarn-apps/apps</p>
<p><img src="../../../img/programming/spark/04_yarn_client.png" class="img-fluid"></p>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>该模式属于yarn client模式，只有driver运行在任务提交的机器上，executor都在大数据的正式机器上运行，所以提交需谨慎些，最好由大数据开发检查后提交。</p>
</div>
</div>
</section>
<section id="yarn-cluster模式" class="level2">
<h2 class="anchored" data-anchor-id="yarn-cluster模式">yarn cluster模式</h2>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>spark<span class="op">-</span>submit <span class="op">--</span>master yarn <span class="op">--</span>deploy<span class="op">-</span>mode cluster <span class="op">--</span>queue llap <span class="op">--</span>name read<span class="op">-</span>hbase<span class="op">-</span>workflow <span class="op">--</span>num<span class="op">-</span>executors <span class="dv">15</span> <span class="op">--</span>executor<span class="op">-</span>cores <span class="dv">2</span> <span class="op">--</span>executor<span class="op">-</span>memory <span class="dv">15</span>g <span class="op">--</span>driver<span class="op">-</span>memory <span class="dv">5</span>g <span class="op">--</span>driver<span class="op">-</span>cores <span class="dv">2</span> <span class="op">--</span>conf <span class="st">"spark.pyspark.python=/usr/bin/python3.5"</span> <span class="op">--</span>conf spark.driver.extraClassPath<span class="op">=/</span>usr<span class="op">/</span>hdp<span class="op">/</span>current<span class="op">/</span>phoenix<span class="op">-</span>client<span class="op">/</span>phoenix<span class="op">-</span>client.jar:<span class="op">/</span>usr<span class="op">/</span>hdp<span class="op">/</span>current<span class="op">/</span>spark2<span class="op">-</span>client<span class="op">/</span>jars<span class="op">/</span>spark<span class="op">-</span>hive_2<span class="fl">.11</span><span class="op">-</span> <span class="op">--</span>conf spark.executor<span class="fl">.2.3.1.3.0.0.0</span><span class="op">-</span><span class="ot">1634.j</span>ar:<span class="op">/</span>usr<span class="op">/</span>hdp<span class="op">/</span>current<span class="op">/</span>spark2<span class="op">-</span>client<span class="op">/</span>jars<span class="op">/</span>spark<span class="op">-</span>examples_2<span class="fl">.11</span><span class="op">-</span><span class="fl">1.6.0</span><span class="op">-</span>typesafe<span class="op">-</span><span class="ot">001.j</span>arextraClassPath<span class="op">=/</span>usr<span class="op">/</span>hdp<span class="op">/</span>current<span class="op">/</span>phoenix<span class="op">-</span>client<span class="op">/</span>phoenix<span class="op">-</span>client.jar:<span class="op">/</span>usr<span class="op">/</span>hdp<span class="op">/</span>current<span class="op">/</span>spark2<span class="op">-</span>client<span class="op">/</span>jars<span class="op">/</span>spark<span class="op">-</span>hive_2<span class="fl">.11</span><span class="op">-</span> <span class="op">--</span>archives <span class="op">/</span>home<span class="op">/</span>klicenbd2<span class="fl">.3.1.3.0.0.0</span><span class="op">-</span><span class="ot">1634.j</span>ar:<span class="op">/</span>usr<span class="op">/</span>hdp<span class="op">/</span>current<span class="op">/</span>spark2<span class="op">-</span>client<span class="op">/</span>jars<span class="op">/</span>spark<span class="op">-</span>examples_2<span class="fl">.11</span><span class="op">-</span><span class="fl">1.6.0</span><span class="op">-</span>typesafe<span class="op">-</span><span class="ot">001.j</span>ar test_pyspark_venv.tar.gz<span class="co">#environment --py-files /data/protobuf_to_dict.py /data/read_hbase.py</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>该模式和yarn client的提交模式在命令上唯一的区别是多了个参数–deploy-mode cluster。界面只有从yarn的任务管理界面找到你自己提交的任务进入了：</p>
<p>yarn的任务管理界面： http://b-bd-cd1-01.local.lan:8088/clusterhttp://b-bd-cd1-01.local.lan:8088/ui2/index.html#/yarn-apps/apps</p>
<p><img src="../../../img/programming/spark/04_yarn_cluster.png" class="img-fluid"></p>
<p>从ui上你会发现driver已经不在你提交任务的机器上了，和executor一样，都在大数据的正式集群机器上。其余区别：driver运行在application master中，负责申请资源和监控executor的任务执行状况，你可以离开提交任务的界面，不会中断任务的执行。–archives /home/klicenbd/jiaoruiqiang/test_pyspark_venv.tar.gz#environment对于之前准备的python运行环境在这里将不适用，因为driver运行在了大数据随机的一台机器上，而不再是你提交任务的机器，所以它会找不到该压缩包，两种办法，要么将虚拟环境压缩包在每台机器都放一个，要么在大数据机器上用</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>pip3 install  packages</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>程序所需要的python包。我采用的后者，相对要方便一点。</p>
<p>用图展示如下（左边yarn cluster，右边yarn client）</p>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>该模式属于yarn cluster模式，driver和executor都在大数据的正式机器上运行，所以提交需谨慎些，最好由大数据开发检查后提交。</p>
</div>
</div>
</section>
<section id="mesos模式" class="level2">
<h2 class="anchored" data-anchor-id="mesos模式">mesos模式</h2>
<p>该模式是spark官方推荐的运行模式。但目前公司未使用，mosos也是一个集群资源管理器，mesos提供粗粒度和细粒度的资源管理模式，但是目前由于细粒度的管理模式相对繁琐，已弃用，保留的粗粒度管理模式和yarn的资源管理很类似，公司选择了yarn来管理，所以这里我们就不细说了，可以自行去查资料了解。</p>
<p>目前从上面4种运行模式来看，pyspark的运行速度都赶不上spark，因为涉及python进程和jvm进程数据交互，所以建议从pyspark熟悉后采用spark来执行，因为api几乎都是一样的。</p>
<section id="spark提交任务参数说明" class="level3">
<h3 class="anchored" data-anchor-id="spark提交任务参数说明">Spark提交任务参数说明</h3>
<ul>
<li><p>–class 入口类名，对于Java和Scala程序来说是包含main()的类的名字，python程序则无须指定该选项</p></li>
<li><p>–master 集群master的地址</p>
<ul>
<li><p>local ：以本地单线程运行Spark</p></li>
<li><p>local[N] ：以本地多个线程运行Spark</p></li>
<li><p>local[*] ：以本地模式运行Spark，线程数等于机器的内核数</p></li>
<li><p>spark://host:port ：连接指定的Standalone集群运行spark</p></li>
<li><p>yarn-client ：以client模式连接到YARN集群，集群位置将通过HADOOP_CONF_DIR环境变量获得</p></li>
<li><p>yarn-cluster ：以cluster模式连接到YARN集群，集群位置将通过HADOOP_CONF_DIR环境变量获得</p></li>
<li><p>mesos://host:port ： 连接指定的Standalone集群运行spark</p></li>
</ul></li>
<li><p>–deploy-mode 程序部署模式</p>
<p>将driver部署到worker节点(cluster模式)或者作为外部客户端部署到本地(client模式)，默认情况下是client模式</p>
<ul>
<li><p>client：执行Spark-submit命令的机器上启动Drive程序</p></li>
<li><p>cluster：会在其中一台工作节点上运行drive程序</p></li>
</ul></li>
<li><p>–jars 第三方发布的Jar包，多个jar包需要用英文的逗号隔开</p></li>
<li><p>–num-executors：executor进程数</p></li>
<li><p>–executor-cores：</p></li>
</ul>
<p>executor进程分配到的核数,单个executor能并发执行task数</p>
<ul>
<li><p>–executor-memory：executor进程分配到的内存大小，根据job需求以及并发数设置</p></li>
<li><p>–py-files：用于上传.py,.zip和.egg的文件</p></li>
<li><p>–driver-memory：指定drive进程进行分配内存大小</p></li>
<li><p>–conf spark.storage.memoryFraction：设置内存用来作为cache的比例(=0.1)</p></li>
<li><p>–conf spark.hadoop.fs.hdfs.impl.disable.cache：禁止使用内存cache(=true)</p></li>
<li><p>–conf spark.default.parallelism: 控制Spark中的分布式shuffle过程默认使用的task数量，默认Others: total number of cores on all executor nodes or 2, whichever is larger，每一个CPU核(core)分配2-3个任务(=400)</p></li>
<li><p>–conf spark.yarn.executor.memoryOverhead: JVM进程中除Java堆以外占用的空间大小，包括方法区(永久代)、Java虚拟机栈、本地方法栈、JVM进程本身所用的内存、直接内存(Direct Memory)等(=5000)</p></li>
<li><p>–conf spark.kryoserializer.buffer: 在每一个work的每一个core上都有一个初始化的buffer, 这些buffer的最大的可达到conf spark.kryoserializer.buffer.max参数设置的参数</p></li>
<li><p>–conf spark.kryoserializer.buffer.max 用来设置对象序列化占用空间大小(=2000m)</p></li>
<li><p>–conf spark.driver.maxResultSize:表所有分区的序列化运行总的内存限制 ,控制worker送回driver的数据大小，一旦操过该限制，driver会终止执行(=10g)</p></li>
</ul>

<!-- -->
</section>
</section>
<script type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    setTimeout(function() {
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    window.tippy(el, {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    }); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      const id = new URL(ref.getAttribute('href')).hash.replace(/^#/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</main>
<div class="page-navigation page-navigation-docked">
  <div class="nav-page nav-page-previous">
      <a href="../../../rmd/programming/spark/read-mysql.html">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">读取 MySQL</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../rmd/programming/spark/hadoop.html">
        <span class="nav-page-text">hadoop 常用命令</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</div>
</div> <!-- /main column -->
</div> <!-- /row -->
</div> <!-- /container fluid -->


</body></html>