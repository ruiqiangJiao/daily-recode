[
  {
    "href": "about.html",
    "title": "关于",
    "section": "",
    "text": ""
  },
  {
    "href": "index.html",
    "title": "主页",
    "section": "",
    "text": ""
  },
  {
    "href": "refrence.html",
    "title": "参考",
    "section": "",
    "text": "Z-Library: 全球最大的数字图书馆"
  },
  {
    "href": "rmd\\algorithm\\deep_learning\\dive_into_deep_learning.html",
    "title": "动手学深度学习",
    "section": "",
    "text": "《动手学深度学习》: 《Dive into Deep Learning》原书中的MXNet实现改为PyTorch实现\n参考资料\nCSBook"
  },
  {
    "href": "rmd\\algorithm\\other\\geohash.html",
    "title": "geohash",
    "section": "",
    "text": "GeoHash算法详解及其实现\nGeoHash原理和可视化显示\n高德 Geohash\ngeohash 精度"
  },
  {
    "href": "rmd\\algorithm\\other\\slam.html",
    "title": "slam",
    "section": "",
    "text": "slam"
  },
  {
    "href": "rmd\\algorithm\\reinforcement_learning\\about.html",
    "title": "参考文档",
    "section": "",
    "text": "强化学习"
  },
  {
    "href": "rmd\\algorithm\\statistics\\elements_of_statistical_learning.html",
    "title": "The Elements of Statistical Learning",
    "section": "",
    "text": "The Elements of Statistical Learning\nComputer Age Statistical Inference"
  },
  {
    "href": "rmd\\algorithm\\statistics\\isolation_forest.html",
    "title": "孤立森林",
    "section": "",
    "text": "孤立森林\n孤立森林扩展\n代码"
  },
  {
    "href": "rmd\\algorithm\\statistics\\lof.html",
    "title": "局部离群因子",
    "section": "",
    "text": "局部离群因子"
  },
  {
    "href": "rmd\\algorithm\\statistics\\XGboost.html",
    "title": "XGboost",
    "section": "",
    "text": "XGboost"
  },
  {
    "href": "rmd\\programming\\git\\refrence.html",
    "title": "精通Git(第二版简体中文)",
    "section": "",
    "text": "精通Git(第二版简体中文)\nGithub Docs"
  },
  {
    "href": "rmd\\programming\\leetcode\\refrence.html",
    "title": "Leet code 参考资料",
    "section": "",
    "text": "SnailTyan"
  },
  {
    "href": "rmd\\programming\\python\\frequent-function.html#分组排序",
    "title": "常用方法",
    "section": "分组排序",
    "text": "import pandas as pd\ndf = pd.DataFrame({'Sp':['a','b','c','d','e','f'], 'Mt':['s1', 's1', 's2','s2','s2','s3'], 'Value':[1,2,3,4,5,6], 'Count':[3,2,5,10,10,6]})\ndf\n\n\n\n\n  \n    \n      \n      Sp\n      Mt\n      Value\n      Count\n    \n  \n  \n    \n      0\n      a\n      s1\n      1\n      3\n    \n    \n      1\n      b\n      s1\n      2\n      2\n    \n    \n      2\n      c\n      s2\n      3\n      5\n    \n    \n      3\n      d\n      s2\n      4\n      10\n    \n    \n      4\n      e\n      s2\n      5\n      10\n    \n    \n      5\n      f\n      s3\n      6\n      6\n    \n  \n\n\n\n\n\ndf.sort_values('Count', ascending=False).groupby('Mt', as_index=False).first()\n\n\n\n\n  \n    \n      \n      Mt\n      Sp\n      Value\n      Count\n    \n  \n  \n    \n      0\n      s1\n      a\n      1\n      3\n    \n    \n      1\n      s2\n      d\n      4\n      10\n    \n    \n      2\n      s3\n      f\n      6\n      6"
  },
  {
    "href": "rmd\\programming\\python\\frequent-function.html#同一个组进行合并",
    "title": "常用方法",
    "section": "同一个组进行合并",
    "text": "series = df.groupby(by='Mt',as_index=True).apply(lambda p:[','.join(p['Sp'])])\ndf_result = pd.DataFrame({'Mt':series.index, 'Sp': series.values})\ndf_result\n\n\n\n\n  \n    \n      \n      Mt\n      Sp\n    \n  \n  \n    \n      0\n      s1\n      [a,b]\n    \n    \n      1\n      s2\n      [c,d,e]\n    \n    \n      2\n      s3\n      [f]"
  },
  {
    "href": "rmd\\programming\\python\\pytorch.html",
    "title": "pytorch 参考",
    "section": "",
    "text": "pytorch handbook\nPytorch Tutorial"
  },
  {
    "href": "rmd\\programming\\python\\read-mysql.html#代码样例",
    "title": "读取 MySQL",
    "section": "代码样例",
    "text": "# - *- coding= utf-8 -*-\n\nimport pymysql\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\nmysql_config = {\"host\": \"\", \"database\": \"\", \"user\": \"\", \"password\": \"\", \"port\": 2222}\n\n\n# 获取 mysql 数据\ndef get_mysql_data(host, database, user, password, port, sql_cmd):\n    db_con = pymysql.connect(\n        host=host,\n        database=database,\n        user=user,\n        password=password,\n        port=port,\n        charset='utf8')\n    df_data = pd.read_sql(sql_cmd, db_con)\n    return df_data\n\n\ndef reader_sql():\n    engine = create_engine('mysql+pymysql://{0}:{1}@{2}:{3}/{4}?charset=utf8'.format(mysql_config['user'],\n                                                                                     mysql_config['password'],\n                                                                                     mysql_config['host'],\n                                                                                     mysql_config['port'],\n                                                                                     mysql_config['database']))\n    df = pd.read_sql_query(\"select * from table_name limit 10\", engine)\n    return df\n\n\nif __name__ == '__main__':\n    df_mysql_data = get_mysql_data(host=mysql_config['host'],\n                                   database=mysql_config['database'],\n                                   user=mysql_config['user'],\n                                   password=mysql_config['password'],\n                                   port=mysql_config['port'],\n                                   sql_cmd=\"select * from table_name limit 10\")"
  },
  {
    "href": "rmd\\programming\\python\\read-redis-cluster.html#代码样例",
    "title": "读取 Redis 集群",
    "section": "代码样例",
    "text": "# -*- coding=utf-8 -*-\n\nfrom rediscluster import StrictRedisCluster\nimport TerminalLocationReport_pb2 as TerminalLocationReport\n\n\nif __name__ == '__main__':\n    nodes = [{'host': h1, 'port': p1},\n             {'host': h2, 'port': p2},\n             {'host': h3, 'port': p3}]\n    k = 'a:t'\n    redis_conn = StrictRedisCluster(startup_nodes=nodes, decode_responses=False, skip_full_coverage_check=True)\n    redis_data = redis_conn.get(k)\n    lr = TerminalLocationReport.TerminalLocationReport()\n    lr.ParseFromString(redis_data)"
  },
  {
    "href": "rmd\\programming\\R\\about.html#shiny-部署",
    "title": "常用方法",
    "section": "shiny 部署",
    "text": "访问shiny 官网，注册并登录;\n\n\nemail : ruiqiang@my.swjtu.edu.cn\nusername : jiaoruiqiang\n\n\n加载 rsconnect 包，并应用其中的函数 deployApp 进行发布.\n\nThe R Graph Gallery"
  },
  {
    "href": "rmd\\programming\\software\\latex.html",
    "title": "LaTex",
    "section": "",
    "text": "overleaf"
  },
  {
    "href": "rmd\\programming\\software\\vim.html",
    "title": "Vim",
    "section": "",
    "text": "VIM 中文帮助手册\nLearning Vim"
  },
  {
    "href": "rmd\\programming\\spark\\install.html",
    "title": "安装",
    "section": "",
    "text": "pyspark中的python依赖管理\n目前管理pyspark的python依赖有两种方式：\n1.打包虚拟环境成压缩包,用–py-files或者–archives提交压缩虚拟环境。\n2.pyspark自己生成虚拟环境的方式，仅限于yarn client和yarn cluster模式。\nUsing VirtualEnv with PySpark\n我们是本地模式运行spark所以用第一种方式，管理python依赖的方式也有好几种如canda，Virtualenv，pex都可以，可以参考这篇文章：https://。databricks.com/blog/2020/12/22/how-to-manage-python-dependencies-in-pyspark.html我们尝试用比较熟悉的方式Virtualenv 管理依赖。\nHow to Manage Python Dependencies in PySpark\n我们此次用的是python 3.x来创建虚拟环境，进入虚拟环境，安装需要的python包，打包虚拟环境\npython3.5 -m venv pyspark_venv\nsource pyspark_venv/bin/activate\npip install pyarrow pandas venv-pack\nvenv-pack -o pyspark_venv.tar.gz\n\n\n\n\n\n\nWarning\n\n\n\n我们前面提到打包提交的方式可以用–py-files配置，也可以用–archives来配置；用–archives配置时要加上#environment，用–py-files配置时不需要，同时–py-file还支持xxx.zip文件及多个python files，–archives也支持多个文件。\n\n\n\n\n\n\n\n\nWarning\n\n\n\n目前测试发现仅能在虚拟环境内部才能执行带有python依赖的pyspark程序，退出虚拟环境执行不了。这个问题是因为退出虚拟环境提交pyspark用的是python xxxx.py去执行的，本地python命令指向的是python 2.x，而这个2.x的系统python环境比如没有装pandas就会报错：ImportError: No module named pandas。解决这一问题需要在提交命令中加入配置\n\n\n--conf \"spark.pyspark.python=/usr/bin/python3.5\"\n完整的命令是：\nspark-submit --master local[*] --num-executors 20 --executor-cores 2 --executor-memory 10g --driver-memory 6g --driver-cores 2 --conf \"spark.pyspark.python=/usr/bin/python3.5\" --archives /usr/local/lib/python3.5/pyspark_venv.tar.gz#environment /data/read_hbase.py\nWindows平台下单机Spark环境搭建\njar 包\nLearning Spark\nApacheCN"
  },
  {
    "href": "rmd\\programming\\spark\\read-hbase.html#安装包",
    "title": "读取Hbase",
    "section": "安装包",
    "text": "python 包\n\npyspark==3.1.1\nprotobuf==3.17.0\npy4j==0.10.9\n\n\n\njar 包\n\nhbase-common-2.0.0.3.0.0.0-1634.jar\nhbase-client-2.0.0.3.0.0.0-1634.jar\nhbase-mapreduce-2.0.0.3.0.0.0-1634.jar\nhbase-shaded-miscellaneous-2.1.0.jar\nhbase-protocol-2.0.0.3.0.0.0-1634.jar\nhbase-protocol-shaded-2.0.0.3.0.0.0-1634.jar\nhbase-shaded-protobuf-2.1.0.jar\nhbase-shaded-netty-2.1.0.jar\nspark-examples_2.11-1.6.0-typesafe-001.jar\n查询 Hbase 得到的 rowKey 类型 ImmutableBytesWritable 和结果做类型转换成 string\n\n上面的 jar 包放置在 spark-script/lib/python3.7/site-packages/pyspark/jars 目录下面"
  },
  {
    "href": "rmd\\programming\\spark\\read-hbase.html#测试样例",
    "title": "读取Hbase",
    "section": "测试样例",
    "text": "from pyspark.sql import SparkSession\nfrom pyspark import SparkConf\nimport TerminalLocationReport_pb2 as TerminalLocationReportProto\nimport json\nimport sys, os\nimport codecs\n\ndef get_rdd_element(rdd):\n    row_values = json.loads(rdd[1])['value']\n    a = codecs.escape_decode(row_values.encode())[0]                \n    report = TerminalLocationReportProto.TerminalLocationReport()\n    report.ParseFromString(a)\n    return report\n\ndef get_hbase_terminal_info():\n    \"\"\"\n    hbase_conf 中的参数配置\n    zookeeper集群地址 \n    hbase master的元数据信息都存在zookeeper中，读取zookeeper就能锁定你要读的数据在hdfs的哪个位置；\n    指定到zookeeper的hbase原数据目录下去读；读取的表名\n    \n    \"\"\"\n    hbase_conf = {\n                 'hbase.zookeeper.quorum':'b-bd-cd1-01.local.lan,b-bd-cd1-02.local.lan,b-bd-cd1-03.local.lan',                                      \n                 'zookeeper.znode.parent': '/hbase-unsecure',\n                 'hbase.mapreduce.inputtable': ''\n    }\n    \n    \"\"\"\n    newAPIHadoopRDD用sparkContext读取hbase表，依次配置：指定读取表用TableInputFormat类；\n    rowkey的返回类型: 数据返回类型；rowkey返回类型强制转换为string；数据返回类型强制转换为string\n    \"\"\"\n\n    data_rdd = sc.newAPIHadoopRDD('org.apache.hadoop.hbase.mapreduce.TableInputFormat',\n                                   'org.apache.hadoop.hbase.io.ImmutableBytesWritable',\n                                   'org.apache.hadoop.hbase.client.Result',\n                                   'org.apache.spark.examples.pythonconverters.ImmutableBytesWritableToStringConverter',\n                                   'org.apache.spark.examples.pythonconverters.HBaseResultToStringConverter',\n                              conf=hbase_conf)\\\n                  .map(get_rdd_element)\n    return data_rdd\n    \n    \nif __name__ == \"__main__\":\n   # add config\n   conf = SparkConf()                                # 初始化 \n   conf.setAppName('read-hbase')                     # 设置程序名称\n   # 设置 spark 采取 Kryo 序列化\n   conf.set('spark.serializer', 'org.apache.spark.serializer.KryoSerializer')\n   conf.set('spark.kryoserializer.buffer.max', '512m')\n   # spark_session 用于读取多种数据源(mysql、hdfs、Hive)\n   spark_session = SparkSession.builder.config(conf=conf).getOrCreate()\n   # spark_session 中的 sparkContext 对象可读取 hbase、hdfs\n   sc = spark_session.sparkContext\n   rdd_data = get_hbase_terminal_info()\n   print(rdd_data.take(1))"
  },
  {
    "href": "rmd\\programming\\spark\\read-hbase.html#客户端读取",
    "title": "读取Hbase",
    "section": "客户端读取",
    "text": "需要安装的包\n\nhappybase==1.2.0\nply==3.11\nprotobuf==3.13.0\nsix==1.15.0\nthriftpy2==0.4.11\n\n使用hbase thrift 连接hbase，读取数据，python 端使用 happybase 连接服务器\n\n\n测试样例\n# -*- coding=utf-8 -*-\n\nimport happybase\nimport pandas as pd\nimport json\nimport T_pb2 as TReport\n\nHBASE_DATABASE = {'host': \"\", 'port': }\nconnection = happybase.Connection(**HBASE_DATABASE)\nreport = connection.table(name=table_name)\n\ndata_list = range(0, 100)\n\nres = list()\nfor t in data_list:\n    print(t)\n    a = t[::-1]\n    u_list = list()\n    for key, value in report.scan():\n        lr = TReport.T()\n        lr.ParseFromString(value[b'F:C'])\n        print(lr)\n        u = lr.terminal_status.u\n        u_list.extend([uart])\n    res.extend([{'t': t, \"u_len\": len(u_list), \"u_sum\": sum(u_list)}])\ndf_all = pd.DataFrame(res)"
  },
  {
    "href": "rmd\\programming\\spark\\read-mysql.html#测试样例",
    "title": "读取 MySQL",
    "section": "测试样例",
    "text": "# - *- coding= utf-8 -*-\n\nfrom pyspark.sql import SparkSession, SQLContext\nfrom pyspark import SparkConf, SparkContext\n\n\ndef get_mysql():\n    prop = {'user': '', 'password': '', 'driver': 'com.mysql.jdbc.Driver'}\n    url = 'jdbc:mysql://{0}:{1}/{2}'.format(\"host\", \"port\", \"database\")\n    df_terminal_detail = spark_session.read.jdbc(url=url, table='', properties=prop)\n    \n    # df_termianl_detail = spark_session.read.format(\"jdbc\")\\\n    #                                      .option(\"url\", \"jdbc:postgresql:postgres\")\\\n    #                                       .option(\"dbtable\", \"\")\\\n    #                                       .option(\"user\", \"\")\\\n    #                                       .option(\"password\", \"\")\\\n    #                                       .load()\n    return df_terminal_detail\n\n\nif __name__ == '__main__':\n    conf = SparkConf()\n    conf.setAppName('user_behavior')\n    conf.set('spark.serializer', 'org.apache.spark.serializer.KryoSerializer')\n    conf.set('spark.kryoserializer.buffer.max', '512m')\n    spark_session = SparkSession.builder.config(conf=conf).getOrCreate()\n    sc = spark_session.sparkContext\n    df_mysql = get_mysql()"
  },
  {
    "href": "rmd\\programming\\spark\\submit-run-task.html#local模式",
    "title": "服务器提交运行任务",
    "section": "local模式",
    "text": "spark-submit --master local[*] --num-executors 20 --executor-cores 2 --executor-memory 10g --driver-memory 6g --conf \"spark.pyspark.python=/usr/bin/python3.5\" --archives /usr/local/lib/python3.5/pyspark_venv.tar.gz#environment /data/read_hbase.py\n提交命令中使用的–master local[n]或者是–master local[*]，该种模式提交的spark任务只会在当前机器上运行，这种模式运行spark任务，你在机器上看到只有一个进程，日志打出：\n21/08/01 08:28:15 WARN Utils: Service ‘SparkUI’ could not bind on port 4040. Attempting port 4041.\n说明该任务的ui界面默认是4040端口展示，但是4040端口被占用了，所以用了4041端口来启动ui界面，用当前机器ip加端口就能看到该任务的ui图。就我们的机器而言就是ip:4041。进入ui界面，可以看到自己的spark任务的dag图，验证前面所说的只有一个进程，点击executors，就可以看到一共启动了多少个进程去运行该任务：\n可以看到只有一个driver进程，就在b-anal-cd1-01也就是241上启动的，分配了72个cores，计算出来的storge momory是3.2g。看到这个ui，你可能就用疑问了，为什么是72cores，storge momory为什么是3.2g?\n原因是由于我们用的是–master local[*]（local模式启动）其中*代表当前机器有多少core就会启动多少个线程，local[n]表示启动n个，至于storge momory其实是根据spark的内存模型算出来的。local模式下spark-submit进程既当爹又当妈，既是提交任务的client进程，又是driver进程，还是task的executor（spark中driver是用于管理executor，将任务分发给executor执行，executor执行完后通知driver）。所以提交参数中–num-executors 20 –executor-cores 2 –executor-memory 10g这个几个参数是无效的，也是不需要的。–driver-memory 6g设置了driver的总内存6g，算下来storge momory是3.2g。\n该种模式下玩的是单进程，启动多线程的方式来模拟了spark启动多个executor去执行spark的子任务，所以我们需要把driver的内存配大到yarn cluster模式下多个executor内存和的大小。\nspark-submit --master local[*] --driver-memory 100g --conf \"spark.pyspark.python=/usr/bin/python3.5\" --archives /usr/local/lib/python3.5/pyspark_venv.tar.gz#environmen /data/read_hbase.py\n\n\n\n\n\n\n\nWarning\n\n\n\n该模式属于当前机器本地运行spark任务，仅限于数据比较少，几十个g或者是测试的情况下"
  },
  {
    "href": "rmd\\programming\\spark\\submit-run-task.html#standalone模式",
    "title": "服务器提交运行任务",
    "section": "standalone模式",
    "text": "spark-submit --master --num-executors 15 --total-executor-cores=40 --executor-cores 2 --executor-spark://b-anal-cd1-01:7077memory 15 --driver-memory 5g --conf \"spark.pyspark.python=/usr/bin/python3.5\"--archives  --py-files /data/*.py /data/read_hbase.py\nspark的集群模式，什么意思啦？也就是说spark这个组件可以维护一个集群，通过spark-submit可以将任务提交到集群中去运行。该集群由一个master和多个worker组成，submit提交任务给master后，master会根据参数在自己管理的所有worker节点上启动30个executor和一个driver，同时worker会给每一个启动的executor分配15g的内存。\n目前只有一台服务器 A，在上面启动了master和worker，那么就维护了一个只有一台机器的spark集群，master和worker1都是当前机器，worker1就会启动30个executor和一个driver，同时worker会给每一个启动的executor分配15g的内存\n如果再分配一台服务器B，我在上面只需启动worker就可以，那么就维护了一个两台机器的spark集群，master和worker1在A机器上worker2在B机器上，你提交任务过来的时候，worker1和worker2就会一起去完成启动30个executor和一个driver，同时worker会给每一个启动的executor分配15g的内存。可能就是worker1启动一个driver和10个executor，worker2启动剩余的20个executor。\n使用该种模式，首先需要去创建一个spark集群, 然后在/usr/hdp/3.0.0.0-1634/spark2目录下，\n\n执行./sbin/start-master.sh启动master，可以在默认端口ip:8080的master的ui界面看到集群状况\n执行./sbin/start-slave.sh -c 30 -m 200G 启动worker，可以使用机器的30个core和200个g的内存用来分配给spark://ip:7077executor或driver执行任务。执行后在master的ui上可以看到一个worker加入了集群：\n\n\n如果还有别的机器，在那台机器上也执行一下：./sbin/start-slave.sh-c 30 -m 200G，ui上又一个worker也加入了spark://ip:7077进来。从例子看，目前这个集群最多只有200个g的内存和30个core提供任务计算，如果都用完，任务将提交不了。\n该模式下用到了–total-executor-cores=40和–executor-cores 2来控制executor的数量，参数表示总共用40个cores，每个executor用2个cores，所以会启动20个executor。而–num-executors参数也就没有效果了\nspark-submit --master--total-executor-cores=40 --executor-cores 2 --executor-memory 15g --driver-spark://b-anal-cd1-01:7077memory 5g --conf \"spark.pyspark.python=/usr/bin/python3.5\"--archives /home/test_pyspark_venv.tar.gz#environment --py-files /data/protobuf_to_dict.py /data/read_hbase.py"
  },
  {
    "href": "rmd\\programming\\spark\\submit-run-task.html#yarn-client模式",
    "title": "服务器提交运行任务",
    "section": "yarn client模式",
    "text": "spark-submit --master yarn --queue llap --name read-hbase-workflow --num-executors 15 --executor-cores 2 --executor-memory 15g --driver-memory 5g --conf \"spark.pyspark.python=/usr/bin/python3.5\" --conf spark.driver.extraClassPath=/usr/hdp/current/phoenix-client/phoenix-client.jar:/usr/hdp/current/spark2-client/jars/spark-hive_2.11-2.3.1.3.0.0.0-1634.jar:/usr/hdp/current/spark2- --conf spark.executor.extraClassPath=/usr/hdp/current/phoenix-clientclient/jars/spark-examples_2.11-1.6.0-typesafe-001.jar/phoenix-client.jar:/usr/hdp/current/spark2-client/jars/spark-hive_2.11-2.3.1.3.0.0.0-1634.jar:/usr/hdp/current/spark2-client --archives /home/klicenbd/jiaoruiqiang/test_pyspark_venv.tar.gz#environment --/jars/spark-examples_2.11-1.6.0-typesafe-001.jarpy-files /data/protobuf_to_dict.py /data/read_hbase.py\n这种区别于spark集群模式，首先是–master yarn参数改变，设置了–queue llap参数表示该任务提交到yarn后用llap对列的资源，也可以写成offline，具体有没有资源要看一下yarn的资源状况\nhttp://b-bd-cd1-01.local.lan:8088/ui2/index.html#/yarn-queues/root 还对spark.driver.extraClassPath和spark.executor.extraClassPath做了设置,表示driver和executor启动的时候需要把这些jar包拷到自己的容器里面去用\nyarn和spark集群模式的区别是，yarn是一个对机器资源的管理者，当提交参数是–master yarn时，–deploy-mode参数没写默认时client，表示用yarn client模式启动任务。此时提交的任务就会向yarn的一个做资源管理的组件resource manager申请你提交参数中的资源，比如我们的命令就是：我需要启动15个executor，每个executor需要分15g的内存，driver分5g的内存。resource manager就会给yarn的另一个管理集群机器的组件node manager发送指令，选择一个在当前node manager机器上启动application master，再向node managerresource manager注册。接着applicati再去resource manager申请executor的资源，申请到后与对应机器的node manager联系，启动executor，启动好后driver分发任务给on masterexecutor执行。区别于spark集群模式，向自己集群中的master提交并申请资源，同时worker节点去启动executor。\nclient模式的默认端口也是4040，打开后就可以看到当前任务的ui界面了，我们还是来看executors界面：\nyarn和spark集群模式的区别是，yarn是一个对机器资源的管理者，当提交参数是–master yarn时，–deploy-mode参数没写默认时client，表示用yarn client模式启动任务。此时提交的任务就会向yarn的一个做资源管理的组件resource manager申请你提交参数中的资源，比如我们的命令就是：我需要启动15个executor，每个executor需要分15g的内存，driver分5g的内存。resource manager就会给yarn的另一个管理集群机器的组件node manager发送指令，选择一个在当前node manager机器上启动application master，再向node managerresource manager注册。接着applicati再去resource manager申请executor的资源，申请到后与对应机器的node manager联系，启动executor，启动好后driver分发任务给on masterexecutor执行。区别于spark集群模式，向自己集群中的master提交并申请资源，同时worker节点去启动executor。\nclient模式的默认端口也是4040，打开后就可以看到当前任务的ui界面了，我们还是来看executors界面\n可以和上面的spark集群模式的yarn做对比：–num-executors 15 –executor-cores 2 –executor-memory 15g –driver-memory 5g这几个参数一样申请到的资源也是一样的。区别在于yarn client模式下，只有driver容器（进程）启动在当前机器上，其余的executor已经在大数据的其他机器上了。这样读取数据的速度相对更快，因为要读取的数据很可能和executor在一台机器上，所以更快。\n同时提交的任务已经可以在yarn的管理任务列表中看到了，：–name read-hbase-workflow名字和这个参数设置的一样，也可以从yarn界面点进去了yarn的任务管理界面：或http://b-bd-cd1-01.local.lan:8088/clusterhttp://b-bd-cd1-01.local.lan:8088/ui2/index.html#/yarn-apps/apps\n\n\n\n\n\n\n\nWarning\n\n\n\n该模式属于yarn client模式，只有driver运行在任务提交的机器上，executor都在大数据的正式机器上运行，所以提交需谨慎些，最好由大数据开发检查后提交。"
  },
  {
    "href": "rmd\\programming\\spark\\submit-run-task.html#yarn-cluster模式",
    "title": "服务器提交运行任务",
    "section": "yarn cluster模式",
    "text": "spark-submit --master yarn --deploy-mode cluster --queue llap --name read-hbase-workflow --num-executors 15 --executor-cores 2 --executor-memory 15g --driver-memory 5g --driver-cores 2 --conf \"spark.pyspark.python=/usr/bin/python3.5\" --conf spark.driver.extraClassPath=/usr/hdp/current/phoenix-client/phoenix-client.jar:/usr/hdp/current/spark2-client/jars/spark-hive_2.11- --conf spark.executor.2.3.1.3.0.0.0-1634.jar:/usr/hdp/current/spark2-client/jars/spark-examples_2.11-1.6.0-typesafe-001.jarextraClassPath=/usr/hdp/current/phoenix-client/phoenix-client.jar:/usr/hdp/current/spark2-client/jars/spark-hive_2.11- --archives /home/klicenbd2.3.1.3.0.0.0-1634.jar:/usr/hdp/current/spark2-client/jars/spark-examples_2.11-1.6.0-typesafe-001.jar test_pyspark_venv.tar.gz#environment --py-files /data/protobuf_to_dict.py /data/read_hbase.py\n该模式和yarn client的提交模式在命令上唯一的区别是多了个参数–deploy-mode cluster。界面只有从yarn的任务管理界面找到你自己提交的任务进入了：\nyarn的任务管理界面： http://b-bd-cd1-01.local.lan:8088/clusterhttp://b-bd-cd1-01.local.lan:8088/ui2/index.html#/yarn-apps/apps\n\n从ui上你会发现driver已经不在你提交任务的机器上了，和executor一样，都在大数据的正式集群机器上。其余区别：driver运行在application master中，负责申请资源和监控executor的任务执行状况，你可以离开提交任务的界面，不会中断任务的执行。–archives /home/klicenbd/jiaoruiqiang/test_pyspark_venv.tar.gz#environment对于之前准备的python运行环境在这里将不适用，因为driver运行在了大数据随机的一台机器上，而不再是你提交任务的机器，所以它会找不到该压缩包，两种办法，要么将虚拟环境压缩包在每台机器都放一个，要么在大数据机器上用\npip3 install  packages\n程序所需要的python包。我采用的后者，相对要方便一点。\n用图展示如下（左边yarn cluster，右边yarn client）\n\n\n\n\n\n\nWarning\n\n\n\n该模式属于yarn cluster模式，driver和executor都在大数据的正式机器上运行，所以提交需谨慎些，最好由大数据开发检查后提交。"
  },
  {
    "href": "rmd\\programming\\spark\\submit-run-task.html#mesos模式",
    "title": "服务器提交运行任务",
    "section": "mesos模式",
    "text": "该模式是spark官方推荐的运行模式。但目前公司未使用，mosos也是一个集群资源管理器，mesos提供粗粒度和细粒度的资源管理模式，但是目前由于细粒度的管理模式相对繁琐，已弃用，保留的粗粒度管理模式和yarn的资源管理很类似，公司选择了yarn来管理，所以这里我们就不细说了，可以自行去查资料了解。\n目前从上面4种运行模式来看，pyspark的运行速度都赶不上spark，因为涉及python进程和jvm进程数据交互，所以建议从pyspark熟悉后采用spark来执行，因为api几乎都是一样的。"
  },
  {
    "href": "rmd\\programming\\SQL\\MySQL.html#合并两个-sql-查询",
    "title": "MySQL",
    "section": "合并两个 SQL 查询",
    "text": "SELECT get.daytime, get.data as get, xh.data as xh \n        FROM ( \n                SELECT daytime, sum(get_sum) as data \n                FROM yuanbao_get \n                group by daytime \n                order by daytime \n        ) as get, \n        ( \n                SELECT daytime, sum(xh_sum) as data \n                FROM yuanbao_xh \n                group by daytime \n                order by daytime \n        ) as xh \n        where get.daytime = xh.daytime"
  },
  {
    "href": "rmd\\programming\\SQL\\MySQL.html#每日新增",
    "title": "MySQL",
    "section": "每日新增",
    "text": "select\n  min_date,count(distinct user_id) as per_day_new\nfrom\n(\n  select\n    user_id,\n    min(log_date) as min_date\n  from table\n  group by user_id\n)t\ngroup by min_date"
  },
  {
    "href": "rmd\\programming\\python\\frequent-function.html#展示表格",
    "title": "常用方法",
    "section": "展示表格",
    "text": "import pandas as pd\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\npd.set_option('display.float_format',lambda x : '%.2f' % x)\n\n%%HTML\n<style type=\"text/css\">\ntable.dataframe td, table.dataframe th {\n    border: 1px  black solid !important;\n  color: black !important;\n}\n</style>\npandarallel\nparatext\nPython 数据分析\n颜色色卡"
  },
  {
    "href": "rmd\\programming\\python\\frequent-function.html#jupyter-note-展示表格",
    "title": "常用方法",
    "section": "jupyter note 展示表格",
    "text": "import pandas as pd\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n%%HTML\n<style type=\"text/css\">\ntable.dataframe td, table.dataframe th {\n    border: 1px  black solid !important;\n  color: black !important;\n}\n</style>"
  },
  {
    "href": "rmd\\programming\\python\\frequent-function.html#编辑器显示问题",
    "title": "常用方法",
    "section": "编辑器显示问题",
    "text": "打印非科学计数\n\n\npd.set_option('display.float_format',lambda x : '%.2f' % x)"
  },
  {
    "href": "rmd\\programming\\python\\frequent-function.html#参考",
    "title": "常用方法",
    "section": "参考",
    "text": "pandarallel\nparatext\nPython 数据分析\n颜色色卡\n读取 druid"
  }
]